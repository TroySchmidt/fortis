import re
import numpy as np
import pandas as pd
from fortis.engine.models.abstract_building_points import AbstractBuildingPoints
from fortis.engine.vulnerability.abstract_vulnerability_function import (
    AbstractVulnerabilityFunction,
)
import importlib.resources as pkg_resources

class DefaultFloodFunction(AbstractVulnerabilityFunction):
    def __init__(
        self,
        buildings: AbstractBuildingPoints,
        flood_type,
    ):
        """
        Initializes a DefaultFloodFunction object.

        Args:
            buildings (BuildingPoints): BuildingPoints object.
            flood_type (str): The type of flood to analyze (R, CV, CA).
        """
        self.flood_type = flood_type
        self.buildings = buildings

        # Use package resources to load CSV files from the fortis_data package.
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flBldgDmgFn.csv")
            .open("r", encoding="utf-8-sig") as bdf_file
        ):
            self.bdf = pd.read_csv(bdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flContDmgFn.csv")
            .open("r", encoding="utf-8-sig") as cdf_file
        ):
            self.cdf = pd.read_csv(cdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flInvDmgFn.csv")
            .open("r", encoding="utf-8-sig") as idf_file
        ):
            self.idf = pd.read_csv(idf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flDmgXRef.csv")
            .open("r", encoding="utf-8-sig") as xdf_file
        ):
            self.xdf = pd.read_csv(xdf_file)

        # index is used later so very important these are set.
        self.bdf = self.bdf.set_index("BldgDmgFnID")
        self.cdf = self.cdf.set_index("ContDmgFnId")
        self.idf = self.idf.set_index("InvDmgFnId")
        self.xdf = self.xdf.set_index(
            [
                "Occupancy",
                "Basement",
                "StoriesMin",
                "StoriesMax",
                "HazardR",
                "HazardCV",
                "HazardCA",
            ]
        )

        # self.xRefExecuted = False

    def get_damage_id_from_xref(self, occupancy, basement, stories, dmgIdField):
        """
        Retrieves the damage ID from the cross reference table.
        Args:
            occupancy (str): The occupancy type.
            basement (int): The basement type.
            stories (int): The number of stories.
            dmgIdField (str): The damage ID field to retrieve.

        Returns:
            int: The damage ID
        """
        # Retrieve function ID from cross reference table
        # Handle special case for RES3 occupancy
        if occupancy.startswith("RES3") and not occupancy[-1].isdigit():
            occupancy = occupancy[:-1]

        conditions = [
            (occupancy == self.xdf.index.get_level_values("Occupancy")),
            (basement == self.xdf.index.get_level_values("Basement")),
            (stories >= self.xdf.index.get_level_values("StoriesMin")),
            (stories <= self.xdf.index.get_level_values("StoriesMax")),
        ]

        # This is incorrect.  It is riverine or coastal.  If coastal it uses all three based on depth (wave action)
        if self.flood_type == "R":
            conditions.append(self.xdf.index.get_level_values("HazardR") == 1)
        elif self.flood_type == "CV":
            conditions.append(self.xdf.index.get_level_values("HazardCV") == 1)
        elif self.flood_type == "CA":
            conditions.append(self.xdf.index.get_level_values("HazardCA") == 1)

        self.xRef = self.xdf[np.logical_and.reduce(conditions)]

        return (
            0
            if len(self.xRef[dmgIdField].values) == 0
            else (
                self.xRef[dmgIdField].values[0]
                if not pd.isna(self.xRef[dmgIdField].values[0])
                else 0
            )
        )

    def apply_damage_percentages(self):
         # Add ouptut columns to building_points
        fields = self.buildings.fields

        # Already have the three IDs identified.
        self._interpolate_from_lookup(self.bdf, fields.depth_in_structure, fields.building_damage_percent, fields.bddf_id)
        self._interpolate_from_lookup(self.cdf, fields.depth_in_structure, fields.content_damage_percent, fields.cddf_id)
        if fields.iddf_id in self.buildings.gdf.columns:
            self._interpolate_from_lookup(self.idf, fields.depth_in_structure, fields.inventory_damage_percent, fields.iddf_id)


    def apply_damage_percentages2(self):
        """
        Gathers the damage percentages for building, content, and inventory

        Args:

        Returns:

        """
        # Add ouptut columns to building_points
        fields = self.buildings.fields
        gdf = self.buildings.gdf

        gdf["BuildingDamagePct"] = None

        # Handle depth value, calculate corresponding upper/lower indices (field names)
        depths = gdf[self.buildings.fields.depth_in_structure].clip(lower=-4, upper=24)
        l_indices = (
            "ft"
            + depths.abs().floordiv(1).astype(int).astype(str).str.zfill(2)
            + depths.lt(0).map({True: "m", False: ""})
        )
        u_indices = (
            "ft"
            + np.ceil(depths.abs()).astype(int).astype(str).str.zfill(2)
            + depths.lt(0).map({True: "m", False: ""})
        )
        fracs = depths - depths.floordiv(1)

        # Retrieve function IDs from cross reference table
        bldgXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "BldgDmgFnId",
            ),
            axis=1,
        )
        contXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "ContDmgFnId",
            ),
            axis=1,
        )
        invXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "InvDmgFnId",
            ),
            axis=1,
        )

        # Update building points with cross reference IDs if not provided
        gdf[fields.bddf_id] = gdf[fields.bddf_id].apply(
            lambda x: bldgXrefIds[x.name] if pd.isna(x) or x == 0 else x
        )
        gdf[fields.cddf_id] = gdf[fields.cddf_id].apply(
            lambda x: contXrefIds[x.name] if pd.isna(x) or x == 0 else x
        )
        if fields.iddf_id in gdf.columns:
            gdf[fields.iddf_id] = gdf.apply(
                lambda row: invXrefIds[row.name]
                if pd.isna(row[fields.iddf_id]) or row[fields.iddf_id] == 0
                else row[fields.iddf_id],
                axis=1,
            )
        else:
            gdf[fields.iddf_id] = 0

        gdf[fields.building_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.bddf_id, self.bdf
        )
        gdf[fields.content_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.cddf_id, self.cdf
        )
        gdf[fields.inventory_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.iddf_id, self.idf
        )

    def _calculate_damage_pct(self, l_indices, u_indices, fracs, df, dmg_field, dmg_df):
        lower_values = df.apply(
            lambda row: dmg_df.at[row[dmg_field], l_indices[row.name]]
            if row[dmg_field] != 0
            else 0,
            axis=1,
        )
        upper_values = df.apply(
            lambda row: dmg_df.at[row[dmg_field], u_indices[row.name]]
            if row[dmg_field] != 0
            else 0,
            axis=1,
        )
        return lower_values + fracs * (upper_values - lower_values)

    def _interpolate_from_lookup(self, lookup_df: pd.DataFrame, flooddepth_col: str, result_col: str, id_col_gdf: str):
        """
        Interpolates values from a lookup table, handling negative columns,
        matching by an ID column in the gdf to a unique ID column in the lookup_df,
        and adds the result to a specified column. Modifies the gdf in-place.

        Args:
            lookup_df: Dataframe with columns like 'ft04m', 'ft00', 'ft01', etc.,
                and a *unique* ID column.  The ID column should be set as the index.
            flooddepth_col: Name of the flood depth column in gdf.
            result_col: Name of the column in gdf to store the interpolated result.
            id_col_gdf: Name of the ID column in the gdf.

        Returns:
            None (Modifies the gdf in-place)
        """

        gdf = self.buildings.gdf

        # Check if the ID columns exist
        if id_col_gdf not in gdf.columns:
            raise ValueError(f"ID column '{id_col_gdf}' not found in gdf.")

        if not lookup_df.index.is_unique:
            raise ValueError("The index of lookup_df must be unique.")
            
        # Pre-compile the regex for efficiency
        col_name_pattern = re.compile(r"^ft(\d+)(m)?$")

        # Create mapping dictionaries *outside* the loop
        def col_name_to_int(col_name):
            match = col_name_pattern.match(col_name)
            if match:
                value = int(match.group(1))
                return -value if match.group(2) == 'm' else value
            return col_name  # Return original if no match

        col_to_int_map = {
            col: col_name_to_int(col)
            for col in lookup_df.columns
            if isinstance(col_name_to_int(col), int) # only map columns we can convert.
        }
        int_to_col_map = {value: key for key, value in col_to_int_map.items()}

        # --- Main Interpolation Logic ---
        gdf[result_col] = np.nan  # Initialize the result column

        # Iterate over the unique IDs in the GDF
        for unique_id in gdf[id_col_gdf].unique():

            # Filter the gdf for the current ID (this is our primary subset)
            gdf_subset = gdf[gdf[id_col_gdf] == unique_id]

            # Efficiently check if the ID exists in the lookup_df
            if unique_id not in lookup_df.index:
                continue  # Skip if ID is not found
            lookup_row = lookup_df.loc[unique_id] # a Series

            # Get the valid lookup ints *for this row*
            lookup_ints = np.array(sorted([col_to_int_map[col] for col in lookup_row.index if col in col_to_int_map]))

            # 2. Find Floor and Ceiling (using the gdf_subset)
            flood_values = gdf_subset[flooddepth_col].to_numpy()
            # Use 'side=right' to handle the case where flood_value is exactly an integer.
            insertion_points = np.searchsorted(lookup_ints, flood_values, side='right')
            #Clip insertion points.  This is very important.
            insertion_points = np.clip(insertion_points, 1, len(lookup_ints) -1)
            
            floor_indices = insertion_points - 1
            ceil_indices = insertion_points

            floor_ints = lookup_ints[floor_indices]
            ceil_ints = lookup_ints[ceil_indices]
            
            # Map integer indices back to column names, handling potential missing mappings
            floor_cols = [int_to_col_map.get(i) for i in floor_ints]
            ceil_cols = [int_to_col_map.get(i) for i in ceil_ints]


            # Handle edge cases:  If the flood depth is exactly on a lookup value, use that value directly.
            floor_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in floor_cols])
            ceil_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in ceil_cols])


            # 4. Interpolate
            interp_factor = np.where(
                (ceil_ints - floor_ints) != 0,
                (flood_values - floor_ints) / (ceil_ints - floor_ints),
                0  # Avoid division by zero
            )
            interp_result = floor_lookup_values + interp_factor * (ceil_lookup_values - floor_lookup_values)

            # Assign to the *original* gdf using the index from the subset.
            gdf.loc[gdf_subset.index, result_col] = interp_result  