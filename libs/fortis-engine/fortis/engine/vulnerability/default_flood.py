import re
import numpy as np
import pandas as pd
from fortis.engine.models.abstract_building_points import AbstractBuildingPoints
from fortis.engine.vulnerability.abstract_vulnerability_function import (
    AbstractVulnerabilityFunction,
)
import importlib.resources as pkg_resources

class DefaultFloodFunction(AbstractVulnerabilityFunction):
    def __init__(
        self,
        buildings: AbstractBuildingPoints,
        flood_type,
    ):
        """
        Initializes a DefaultFloodFunction object.

        Args:
            buildings (BuildingPoints): BuildingPoints object.
            flood_type (str): The type of flood to analyze (R, CV, CA).
        """
        self.flood_type = flood_type
        self.buildings = buildings

        # Use package resources to load CSV files from the fortis_data package.
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flBldgDmgFn.csv")
            .open("r", encoding="utf-8-sig") as bdf_file
        ):
            self.bdf = pd.read_csv(bdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flContDmgFn.csv")
            .open("r", encoding="utf-8-sig") as cdf_file
        ):
            self.cdf = pd.read_csv(cdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flInvDmgFn.csv")
            .open("r", encoding="utf-8-sig") as idf_file
        ):
            self.idf = pd.read_csv(idf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flDmgXRef.csv")
            .open("r", encoding="utf-8-sig") as xdf_file
        ):
            self.xdf = pd.read_csv(xdf_file)

        # index is used later so very important these are set.
        self.bdf = self.bdf.set_index("BldgDmgFnId")
        self.cdf = self.cdf.set_index("ContDmgFnId")
        self.idf = self.idf.set_index("InvDmgFnId")
        self.xdf = self.xdf.set_index(
            [
                "Occupancy",
                "Basement",
                "StoriesMin",
                "StoriesMax",
                "HazardR",
                "HazardCV",
                "HazardCA",
            ]
        )
        
        # self.xRefExecuted = False

    def get_damage_id_from_xref(self, occupancy, basement, stories, dmgIdField):
        """
        Retrieves the damage ID from the cross reference table.
        Args:
            occupancy (str): The occupancy type.
            basement (int): The basement type.
            stories (int): The number of stories.
            dmgIdField (str): The damage ID field to retrieve.

        Returns:
            int: The damage ID
        """
        # Retrieve function ID from cross reference table
        # Handle special case for RES3 occupancy
        if occupancy.startswith("RES3") and not occupancy[-1].isdigit():
            occupancy = occupancy[:-1]

        conditions = [
            (occupancy == self.xdf.index.get_level_values("Occupancy")),
            (basement == self.xdf.index.get_level_values("Basement")),
            (stories >= self.xdf.index.get_level_values("StoriesMin")),
            (stories <= self.xdf.index.get_level_values("StoriesMax")),
        ]

        # This is incorrect.  It is riverine or coastal.  If coastal it uses all three based on depth (wave action)
        if self.flood_type == "R":
            conditions.append(self.xdf.index.get_level_values("HazardR") == 1)
        elif self.flood_type == "CV":
            conditions.append(self.xdf.index.get_level_values("HazardCV") == 1)
        elif self.flood_type == "CA":
            conditions.append(self.xdf.index.get_level_values("HazardCA") == 1)

        self.xRef = self.xdf[np.logical_and.reduce(conditions)]

        return (
            0
            if len(self.xRef[dmgIdField].values) == 0
            else (
                self.xRef[dmgIdField].values[0]
                if not pd.isna(self.xRef[dmgIdField].values[0])
                else 0
            )
        )

    def apply_damage_ids_vectorized(self):
        """
        Applies damage IDs to the buildings GeoDataFrame in a vectorized manner.
        """
        fields = self.buildings.fields
        gdf = self.buildings.gdf

        # Handle special case for RES3 occupancy
        gdf['OccupancyClean'] = gdf[fields.occupancy_type].str.replace(r'RES3(?=[^\d])', 'RES3', regex=True)

        # Ensure all necessary columns exist
        for col in [fields.occupancy_type, fields.number_stories, fields.bddf_id, fields.cddf_id, fields.iddf_id, fields.flood_depth]:
            if col not in gdf.columns:
                if col == fields.iddf_id:
                    # Optional field, can be skipped if not present.
                    continue
                raise ValueError(f"Column '{col}' not found in gdf.")

        # Map foundation_type to basement (0 or 1)
        gdf['Basement'] = gdf[fields.foundation_type].isin(['B', 4]).astype(int)
        
        # Apply damage IDs in depth-based subsets
        self._apply_damage_ids_for_subset(gdf, fields, "CV", gdf[fields.flood_depth] >= 6)
        self._apply_damage_ids_for_subset(gdf, fields, "CA", (gdf[fields.flood_depth] >= 3) & (gdf[fields.flood_depth] < 6))
        self._apply_damage_ids_for_subset(gdf, fields, "R", gdf[fields.flood_depth] < 3)

        #clean up temp column
        del gdf["OccupancyClean"]
    
    def _apply_damage_ids_for_subset(self, gdf, fields, hazard_type, subset_mask):
        """
        Applies damage IDs to a subset of the buildings GeoDataFrame based on hazard type.

        Args:
            gdf: The GeoDataFrame.
            fields: BuildingMapping fields.
            hazard_type: "R", "CA", or "CV".
            subset_mask: Boolean mask for the subset of rows.
        """
        # Early Exit if subset is empty.
        if not subset_mask.any():
            return

        # Create a MultiIndex from the GeoDataFrame columns for efficient matching
        gdf_multi_index = pd.MultiIndex.from_arrays(
            [
                gdf.loc[subset_mask, "OccupancyClean"],
                gdf.loc[subset_mask, "Basement"].astype(int),  # Cast boolean to int for matching
                gdf.loc[subset_mask, fields.number_stories],
            ],
            names=["Occupancy", "Basement", "Stories"],
        )

        # Prepare the hazard match.
        hazard_match = self.xdf.index.get_level_values(f"Hazard{hazard_type}") == 1
        
        # Prepare common vectorized conditions
        stories_match = (
            (gdf_multi_index.get_level_values("Stories").to_numpy()[:, np.newaxis] >= self.xdf.index.get_level_values("StoriesMin").to_numpy()) &
            (gdf_multi_index.get_level_values("Stories").to_numpy()[:, np.newaxis] < self.xdf.index.get_level_values("StoriesMax").to_numpy())
        )

        occupancy_match = gdf_multi_index.get_level_values("Occupancy").to_numpy()[:, np.newaxis] == self.xdf.index.get_level_values("Occupancy").to_numpy()
        basement_match = gdf_multi_index.get_level_values("Basement").to_numpy()[:, np.newaxis] == self.xdf.index.get_level_values("Basement").to_numpy()

        # Iterate over the damage ID fields
        for dmgIdField, output_column in [
            ("BldgDmgFnId", fields.bddf_id),
            ("ContDmgFnId", fields.cddf_id) if fields.cddf_id in gdf.columns else (None, None),
            ("InvDmgFnId", fields.iddf_id) if fields.iddf_id in gdf.columns else (None, None),
        ]:
            if dmgIdField is None:
                continue

            # Combine all conditions for vectorized lookup
            all_conditions = (occupancy_match & basement_match & stories_match & hazard_match)

            # Lookup the damage ID values from the cross-reference table.
            lookup_values = self.xdf[dmgIdField].values

            # Extract the correct ids.
            matches = lookup_values[all_conditions.argmax(axis=1)]

            # Set the value of those who didn't have any matches to 0.
            matches[all_conditions.sum(axis=1) == 0] = 0

            # Find rows where the current output_column is 0 or NaN in original gdf
            current_values = gdf.loc[subset_mask, output_column].to_numpy()
            update_mask = (current_values == 0) | pd.isnull(current_values)
            
            # Use the mask to update only the appropriate rows in the original.
            gdf.loc[subset_mask & update_mask, output_column] = matches[update_mask]


    def apply_damage_ids_vectorized2(self):
        """
        Applies damage IDs to the buildings GeoDataFrame in a vectorized manner.
        """
        fields = self.buildings.fields
        gdf = self.buildings.gdf

        # Handle special case for RES3 occupancy
        gdf['OccupancyClean'] = gdf[fields.occupancy_type].str.replace(r'RES3(?=[^\d])', 'RES3', regex=True)


        # Ensure all necessary columns exist
        for col in [fields.occupancy_type, fields.number_stories, fields.bddf_id, fields.cddf_id, fields.iddf_id]:
            if col not in gdf.columns:
                if col == fields.iddf_id:
                    # Optional field, can be skipped if not present.
                    continue
                raise ValueError(f"Column '{col}' not found in gdf.")

        # Map foundation_type to basement (0 or 1)
        gdf['Basement'] = gdf[fields.foundation_type].isin(['B', 4]).astype(int)

        # Create a MultiIndex from the GeoDataFrame columns for efficient matching
        gdf_multi_index = pd.MultiIndex.from_arrays(
            [
                gdf["OccupancyClean"],
                gdf["Basement"].astype(int),  # Cast boolean to int for matching
                gdf[fields.number_stories],
            ],
            names=["Occupancy", "Basement", "Stories"],
        )

        # Prepare the Hazard column based on flood_type
        if self.flood_type == "R":
            hazard_col = "HazardR"
        elif self.flood_type == "CV":
            hazard_col = "HazardCV"
        elif self.flood_type == "CA":
            hazard_col = "HazardCA"
        else:
            raise ValueError(f"Invalid flood_type: {self.flood_type}")

        # Vectorized lookup conditions
        hazard_match = self.xdf.index.get_level_values(hazard_col) == 1
        stories_match = (
            (gdf_multi_index.get_level_values("Stories").to_numpy()[:, np.newaxis] >= self.xdf.index.get_level_values("StoriesMin").to_numpy()) &
            (gdf_multi_index.get_level_values("Stories").to_numpy()[:, np.newaxis] < self.xdf.index.get_level_values("StoriesMax").to_numpy())
        )
        
        # Iterate over the damage ID fields
        for dmgIdField, output_column in [
            ("BldgDmgFnId", fields.bddf_id),
            ("ContDmgFnId", fields.cddf_id) if fields.cddf_id in gdf.columns else (None, None),
            ("InvDmgFnId", fields.iddf_id) if fields.iddf_id in gdf.columns else (None, None),
        ]:
            if dmgIdField is None:
                continue

            # Prepare the occupancy and basement values to the correct shape to make comparisons in numpy.
            occupancy_match = gdf_multi_index.get_level_values("Occupancy").to_numpy()[:, np.newaxis] == self.xdf.index.get_level_values("Occupancy").to_numpy()
            basement_match = gdf_multi_index.get_level_values("Basement").to_numpy()[:, np.newaxis] == self.xdf.index.get_level_values("Basement").to_numpy()
          
            # Combine all conditions for vectorized lookup
            all_conditions = (occupancy_match & basement_match & stories_match & hazard_match)
            
            # Lookup the damage ID values from the cross-reference table.
            lookup_values = self.xdf[dmgIdField].values
            
            # Extract the correct ids.
            matches = lookup_values[all_conditions.argmax(axis=1)]
            
            # Set the value of those who didn't have any matches to 0.
            matches[all_conditions.sum(axis=1) == 0] = 0
            
            #Assign the value to the column.
            gdf[output_column] = matches

        #clean up temp column
        del gdf["OccupancyClean"]

    def apply_damage_percentages(self):
         # Add ouptut columns to building_points
        fields = self.buildings.fields

        # Already have the three IDs identified.
        self._interpolate_from_lookup(self.bdf, fields.depth_in_structure, fields.building_damage_percent, fields.bddf_id)
        self._interpolate_from_lookup(self.cdf, fields.depth_in_structure, fields.content_damage_percent, fields.cddf_id)
        if fields.iddf_id in self.buildings.gdf.columns:
            self._interpolate_from_lookup(self.idf, fields.depth_in_structure, fields.inventory_damage_percent, fields.iddf_id)

    def _interpolate_from_lookup(self, lookup_df: pd.DataFrame, flooddepth_col: str, result_col: str, id_col_gdf: str):
        """
        Interpolates values from a lookup table, handling negative columns,
        matching by an ID column in the gdf to a unique ID column in the lookup_df,
        and adds the result to a specified column. Modifies the gdf in-place.

        Args:
            lookup_df: Dataframe with columns like 'ft04m', 'ft00', 'ft01', etc.,
                and a *unique* ID column.  The ID column should be set as the index.
            flooddepth_col: Name of the flood depth column in gdf.
            result_col: Name of the column in gdf to store the interpolated result.
            id_col_gdf: Name of the ID column in the gdf.

        Returns:
            None (Modifies the gdf in-place)
        """

        gdf = self.buildings.gdf

        # Check if the ID columns exist
        if id_col_gdf not in gdf.columns:
            raise ValueError(f"ID column '{id_col_gdf}' not found in gdf.")

        if not lookup_df.index.is_unique:
            raise ValueError("The index of lookup_df must be unique.")
            
        # Pre-compile the regex for efficiency
        col_name_pattern = re.compile(r"^ft(\d+)(m)?$")

        # Create mapping dictionaries *outside* the loop
        def col_name_to_int(col_name):
            match = col_name_pattern.match(col_name)
            if match:
                value = int(match.group(1))
                return -value if match.group(2) == 'm' else value
            return col_name  # Return original if no match

        col_to_int_map = {
            col: col_name_to_int(col)
            for col in lookup_df.columns
            if isinstance(col_name_to_int(col), int) # only map columns we can convert.
        }
        int_to_col_map = {value: key for key, value in col_to_int_map.items()}

        # --- Main Interpolation Logic ---
        gdf[result_col] = np.nan  # Initialize the result column

        # Iterate over the unique IDs in the GDF
        for unique_id in gdf[id_col_gdf].unique():

            # Filter the gdf for the current ID (this is our primary subset)
            gdf_subset = gdf[gdf[id_col_gdf] == unique_id]

            # Efficiently check if the ID exists in the lookup_df
            if unique_id not in lookup_df.index:
                continue  # Skip if ID is not found
            lookup_row = lookup_df.loc[unique_id] # a Series

            # Get the valid lookup ints *for this row*
            lookup_ints = np.array(sorted([col_to_int_map[col] for col in lookup_row.index if col in col_to_int_map]))

            # 2. Find Floor and Ceiling (using the gdf_subset)
            flood_values = gdf_subset[flooddepth_col].to_numpy()
            # Use 'side=right' to handle the case where flood_value is exactly an integer.
            insertion_points = np.searchsorted(lookup_ints, flood_values, side='right')
            #Clip insertion points.  This is very important.
            insertion_points = np.clip(insertion_points, 1, len(lookup_ints) -1)
            
            floor_indices = insertion_points - 1
            ceil_indices = insertion_points

            floor_ints = lookup_ints[floor_indices]
            ceil_ints = lookup_ints[ceil_indices]
            
            # Map integer indices back to column names, handling potential missing mappings
            floor_cols = [int_to_col_map.get(i) for i in floor_ints]
            ceil_cols = [int_to_col_map.get(i) for i in ceil_ints]


            # Handle edge cases:  If the flood depth is exactly on a lookup value, use that value directly.
            floor_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in floor_cols])
            ceil_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in ceil_cols])


            # 4. Interpolate
            interp_factor = np.where(
                (ceil_ints - floor_ints) != 0,
                (flood_values - floor_ints) / (ceil_ints - floor_ints),
                0  # Avoid division by zero
            )
            interp_result = floor_lookup_values + interp_factor * (ceil_lookup_values - floor_lookup_values)

            # Assign to the *original* gdf using the index from the subset.
            gdf.loc[gdf_subset.index, result_col] = interp_result  