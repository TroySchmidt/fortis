import re
import numpy as np
import pandas as pd
from fortis.engine.models.abstract_building_points import AbstractBuildingPoints
from fortis.engine.vulnerability.abstract_vulnerability_function import (
    AbstractVulnerabilityFunction,
)
import importlib.resources as pkg_resources

class DefaultFloodFunction(AbstractVulnerabilityFunction):
    def __init__(
        self,
        buildings: AbstractBuildingPoints,
        flood_type,
    ):
        """
        Initializes a DefaultFloodFunction object.

        Args:
            buildings (BuildingPoints): BuildingPoints object.
            flood_type (str): The type of flood to analyze (R, CV, CA).
        """
        self.flood_type = flood_type
        self.buildings = buildings

        # Use package resources to load CSV files from the fortis_data package.
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flBldgDmgFn.csv")
            .open("r", encoding="utf-8-sig") as bdf_file
        ):
            self.bdf = pd.read_csv(bdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flContDmgFn.csv")
            .open("r", encoding="utf-8-sig") as cdf_file
        ):
            self.cdf = pd.read_csv(cdf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flInvDmgFn.csv")
            .open("r", encoding="utf-8-sig") as idf_file
        ):
            self.idf = pd.read_csv(idf_file)
        with (
            pkg_resources.files("fortis.data")
            .joinpath("flDmgXRef.csv")
            .open("r", encoding="utf-8-sig") as xdf_file
        ):
            self.xdf = pd.read_csv(xdf_file)

        # index is used later so very important these are set.
        self.bdf = self.bdf.set_index("BldgDmgFnID")
        self.cdf = self.cdf.set_index("ContDmgFnId")
        self.idf = self.idf.set_index("InvDmgFnId")
        self.xdf = self.xdf.set_index(
            [
                "Occupancy",
                "Basement",
                "StoriesMin",
                "StoriesMax",
                "HazardR",
                "HazardCV",
                "HazardCA",
            ]
        )

        # self.xRefExecuted = False

    def get_damage_id_from_xref(self, occupancy, basement, stories, dmgIdField):
        """
        Retrieves the damage ID from the cross reference table.
        Args:
            occupancy (str): The occupancy type.
            basement (int): The basement type.
            stories (int): The number of stories.
            dmgIdField (str): The damage ID field to retrieve.

        Returns:
            int: The damage ID
        """
        # Retrieve function ID from cross reference table
        # Handle special case for RES3 occupancy
        if occupancy.startswith("RES3") and not occupancy[-1].isdigit():
            occupancy = occupancy[:-1]

        conditions = [
            (occupancy == self.xdf.index.get_level_values("Occupancy")),
            (basement == self.xdf.index.get_level_values("Basement")),
            (stories >= self.xdf.index.get_level_values("StoriesMin")),
            (stories <= self.xdf.index.get_level_values("StoriesMax")),
        ]

        # This is incorrect.  It is riverine or coastal.  If coastal it uses all three based on depth (wave action)
        if self.flood_type == "R":
            conditions.append(self.xdf.index.get_level_values("HazardR") == 1)
        elif self.flood_type == "CV":
            conditions.append(self.xdf.index.get_level_values("HazardCV") == 1)
        elif self.flood_type == "CA":
            conditions.append(self.xdf.index.get_level_values("HazardCA") == 1)

        self.xRef = self.xdf[np.logical_and.reduce(conditions)]

        return (
            0
            if len(self.xRef[dmgIdField].values) == 0
            else (
                self.xRef[dmgIdField].values[0]
                if not pd.isna(self.xRef[dmgIdField].values[0])
                else 0
            )
        )

    def apply_damage_percentages(self):
         # Add ouptut columns to building_points
        fields = self.buildings.fields

        # Already have the three IDs identified.
        self._interpolate_from_lookup(self.bdf, fields.depth_in_structure, fields.building_damage_percent, fields.bddf_id)
        self._interpolate_from_lookup(self.cdf, fields.depth_in_structure, fields.content_damage_percent, fields.cddf_id)
        if fields.iddf_id in self.buildings.gdf.columns:
            self._interpolate_from_lookup(self.idf, fields.depth_in_structure, fields.inventory_damage_percent, fields.iddf_id)


    def apply_damage_percentages2(self):
        """
        Gathers the damage percentages for building, content, and inventory

        Args:

        Returns:

        """
        # Add ouptut columns to building_points
        fields = self.buildings.fields
        gdf = self.buildings.gdf

        gdf["BuildingDamagePct"] = None

        # Handle depth value, calculate corresponding upper/lower indices (field names)
        depths = gdf[self.buildings.fields.depth_in_structure].clip(lower=-4, upper=24)
        l_indices = (
            "ft"
            + depths.abs().floordiv(1).astype(int).astype(str).str.zfill(2)
            + depths.lt(0).map({True: "m", False: ""})
        )
        u_indices = (
            "ft"
            + np.ceil(depths.abs()).astype(int).astype(str).str.zfill(2)
            + depths.lt(0).map({True: "m", False: ""})
        )
        fracs = depths - depths.floordiv(1)

        # Retrieve function IDs from cross reference table
        bldgXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "BldgDmgFnId",
            ),
            axis=1,
        )
        contXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "ContDmgFnId",
            ),
            axis=1,
        )
        invXrefIds = gdf.apply(
            lambda row: self.get_damage_id_from_xref(
                row[fields.occupancy_type],
                1 if row[fields.foundation_type] == 4 else 0,
                row[fields.number_stories],
                "InvDmgFnId",
            ),
            axis=1,
        )

        # Update building points with cross reference IDs if not provided
        gdf[fields.bddf_id] = gdf[fields.bddf_id].apply(
            lambda x: bldgXrefIds[x.name] if pd.isna(x) or x == 0 else x
        )
        gdf[fields.cddf_id] = gdf[fields.cddf_id].apply(
            lambda x: contXrefIds[x.name] if pd.isna(x) or x == 0 else x
        )
        if fields.iddf_id in gdf.columns:
            gdf[fields.iddf_id] = gdf.apply(
                lambda row: invXrefIds[row.name]
                if pd.isna(row[fields.iddf_id]) or row[fields.iddf_id] == 0
                else row[fields.iddf_id],
                axis=1,
            )
        else:
            gdf[fields.iddf_id] = 0

        gdf[fields.building_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.bddf_id, self.bdf
        )
        gdf[fields.content_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.cddf_id, self.cdf
        )
        gdf[fields.inventory_damage_percent] = self._calculate_damage_pct(
            l_indices, u_indices, fracs, gdf, fields.iddf_id, self.idf
        )

    def _calculate_damage_pct(self, l_indices, u_indices, fracs, df, dmg_field, dmg_df):
        lower_values = df.apply(
            lambda row: dmg_df.at[row[dmg_field], l_indices[row.name]]
            if row[dmg_field] != 0
            else 0,
            axis=1,
        )
        upper_values = df.apply(
            lambda row: dmg_df.at[row[dmg_field], u_indices[row.name]]
            if row[dmg_field] != 0
            else 0,
            axis=1,
        )
        return lower_values + fracs * (upper_values - lower_values)

    def _interpolate_from_lookup(self, lookup_df: pd.DataFrame, flooddepth_col: str, result_col: str, id_col_gdf: str):
        """
        Interpolates values from a lookup table, handling negative columns,
        matching by an ID column in the gdf to a unique ID column in the lookup_df,
        and adds the result to a specified column. Modifies the gdf in-place.

        Args:
            lookup_df: Dataframe with columns like 'ft04m', 'ft00', 'ft01', etc.,
                and a *unique* ID column.  The ID column should be set as the index.
            flooddepth_col: Name of the flood depth column in gdf.
            result_col: Name of the column in gdf to store the interpolated result.
            id_col_gdf: Name of the ID column in the gdf.

        Returns:
            None (Modifies the gdf in-place)
        """

        gdf = self.buildings.gdf

        # Check if the ID columns exist
        if id_col_gdf not in gdf.columns:
            raise ValueError(f"ID column '{id_col_gdf}' not found in gdf.")

        if not lookup_df.index.is_unique:
            raise ValueError("The index of lookup_df must be unique.")
            
        # Pre-compile the regex for efficiency
        col_name_pattern = re.compile(r"^ft(\d+)(m)?$")

        # Create mapping dictionaries *outside* the loop
        def col_name_to_int(col_name):
            match = col_name_pattern.match(col_name)
            if match:
                value = int(match.group(1))
                return -value if match.group(2) == 'm' else value
            return col_name  # Return original if no match

        col_to_int_map = {
            col: col_name_to_int(col)
            for col in lookup_df.columns
            if isinstance(col_name_to_int(col), int) # only map columns we can convert.
        }
        int_to_col_map = {value: key for key, value in col_to_int_map.items()}

        # --- Main Interpolation Logic ---
        gdf[result_col] = np.nan  # Initialize the result column

        # Iterate over the unique IDs in the GDF
        for unique_id in gdf[id_col_gdf].unique():

            # Filter the gdf for the current ID (this is our primary subset)
            gdf_subset = gdf[gdf[id_col_gdf] == unique_id]

            # Efficiently check if the ID exists in the lookup_df
            if unique_id not in lookup_df.index:
                continue  # Skip if ID is not found
            lookup_row = lookup_df.loc[unique_id] # a Series

            # Get the valid lookup ints *for this row*
            lookup_ints = np.array(sorted([col_to_int_map[col] for col in lookup_row.index if col in col_to_int_map]))

            # 2. Find Floor and Ceiling (using the gdf_subset)
            flood_values = gdf_subset[flooddepth_col].to_numpy()
            # Use 'side=right' to handle the case where flood_value is exactly an integer.
            insertion_points = np.searchsorted(lookup_ints, flood_values, side='right')
            #Clip insertion points.  This is very important.
            insertion_points = np.clip(insertion_points, 1, len(lookup_ints) -1)
            
            floor_indices = insertion_points - 1
            ceil_indices = insertion_points

            floor_ints = lookup_ints[floor_indices]
            ceil_ints = lookup_ints[ceil_indices]
            
            # Map integer indices back to column names, handling potential missing mappings
            floor_cols = [int_to_col_map.get(i) for i in floor_ints]
            ceil_cols = [int_to_col_map.get(i) for i in ceil_ints]


            # Handle edge cases:  If the flood depth is exactly on a lookup value, use that value directly.
            floor_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in floor_cols])
            ceil_lookup_values = np.array([lookup_row[col] if col is not None else np.nan for col in ceil_cols])


            # 4. Interpolate
            interp_factor = np.where(
                (ceil_ints - floor_ints) != 0,
                (flood_values - floor_ints) / (ceil_ints - floor_ints),
                0  # Avoid division by zero
            )
            interp_result = floor_lookup_values + interp_factor * (ceil_lookup_values - floor_lookup_values)

            # Assign to the *original* gdf using the index from the subset.
            gdf.loc[gdf_subset.index, result_col] = interp_result

    def _interpolate_from_lookup_almostthere(self, lookup_df: pd.DataFrame, flooddepth_col: str, result_col: str, id_col_gdf: str):
        """
        Interpolates values from a lookup table, handling negative columns,
        matching by an ID column in the gdf to a unique ID column in the lookup_df,
        and adds the result to a specified column. Modifies the gdf in-place.

        Args:
            lookup_df: Dataframe with columns like 'ft04m', 'ft00', 'ft01', etc.,
                    and a *unique* ID column.
            flooddepth_col: Name of the flood depth column in gdf.
            result_col: Name of the column in gdf to store the interpolated result.
            id_col_gdf: Name of the ID column in the gdf.

        Returns:
            None  (Modifies the gdf in-place)
        """

        gdf = self.buildings.gdf

        # Check if the ID columns exist
        if id_col_gdf not in gdf.columns:
            raise ValueError(f"ID column '{id_col_gdf}' not found in gdf.")
       

        # 1. Create a Mapping Dictionary (ONCE, from lookup_df)
        def col_name_to_int(col_name):
            if col_name.endswith('m'):
                return -int(col_name[2:-1])
            else:
                return int(col_name[2:])

        # --- Main Interpolation Logic ---
        gdf[result_col] = np.nan

        # Iterate over the unique IDs IN THE GDF
        for unique_id in gdf[id_col_gdf].unique():

            # Filter the gdf for the current ID (this is our primary subset)
            gdf_subset = gdf[gdf[id_col_gdf] == unique_id]

            if unique_id not in lookup_df.index:
                continue
            lookup_row = lookup_df.loc[unique_id]


            # 1. Create a Mapping Dictionary (for the current ID)
            def col_name_to_int(col_name):
                if col_name.startswith('ft'):
                    if col_name.endswith('m'):
                            return -int(col_name[2:-1])  # Extract number between 'ft' and 'm'
                    else:
                            return int(col_name[2:])  # Extract number after 'ft'
                else:
                    return col_name # Handles cases that don't convert correctly.

            col_to_int_map = {
                col: col_name_to_int(col)
                for col in lookup_df.columns
                if re.match(r"^ft\d{2}.*?$", col)
            }
            # Create the inverted mapping (int_to_col_map)
            int_to_col_map = {value: key for key, value in col_to_int_map.items()}

            lookup_ints = np.array(sorted(col_to_int_map.values()))
            # Check if lookup cols are sequential from -N to N
            expected_ints = np.arange(lookup_ints.min(), lookup_ints.max() + 1)
            if not np.array_equal(lookup_ints, expected_ints):
                raise ValueError(f"Lookup df columns for ID {unique_id} are not sequentially numbered from -N to N")

            # 2. Find Floor and Ceiling (using the gdf_subset)
            flood_values = gdf_subset[flooddepth_col].to_numpy()
            insertion_points = np.searchsorted(lookup_ints, flood_values, side='left')
            insertion_points = np.clip(insertion_points, 1, len(lookup_ints) - 1)
            floor_indices = insertion_points - 1
            ceil_indices = insertion_points

            floor_ints = lookup_ints[floor_indices]
            ceil_ints = lookup_ints[ceil_indices]

            # Get the actual floor and ceil values from the flood depths
            floor_values_from_depth = np.floor(flood_values)
            ceil_values_from_depth = np.ceil(flood_values)
            
            # Create Series (using the gdf_subset's index!)
            floor_col_series = pd.Series(np.vectorize(int_to_col_map.get)(floor_ints), index=gdf_subset.index)
            ceil_col_series = pd.Series(np.vectorize(int_to_col_map.get)(ceil_ints), index=gdf_subset.index)

            # --- CRITICAL: Handle edge cases ---
            exact_matches = floor_values_from_depth == flood_values
            ceil_col_series[exact_matches] = floor_col_series[exact_matches]

            # Handle when flood_values match the integer column name
            is_integer_match = np.isclose(flood_values, np.round(flood_values))

            floor_ints_adjusted = np.where(is_integer_match, floor_ints -1, floor_ints)
            floor_col_series = pd.Series(np.vectorize(int_to_col_map.get)(floor_ints_adjusted), index=gdf_subset.index)
            # --- END CRITICAL: Handle edge cases ---

            # 3. Gather Corresponding Values (using lookup_subset)
            lookup_array = lookup_row.to_numpy()
            row_indices = np.arange(len(gdf_subset))

            floor_col_indices = np.array([lookup_df.columns.get_loc(col) for col in floor_col_series])
            ceil_col_indices = np.array([lookup_df.columns.get_loc(col) for col in ceil_col_series])

            floor_lookup_values = lookup_array[row_indices, floor_col_indices]
            ceil_lookup_values = lookup_array[row_indices, ceil_col_indices]

            # 4. Interpolate
            interp_factor = np.where(
                (ceil_ints - floor_ints) != 0,
                (flood_values - floor_ints) / (ceil_ints - floor_ints),
                0
            )
            interp_result_series = pd.Series(
                floor_lookup_values + interp_factor * (ceil_lookup_values - floor_lookup_values),
                index=gdf_subset.index  # Use gdf_subset's index
            )

            # Handle when flood_values is exactly equal to the integer col:
            interp_result_series = pd.Series(np.where(is_integer_match,
                                                ceil_lookup_values,
                                                interp_result_series),index=gdf_subset.index)

            # Assign to the *original* gdf using the index from the subset.
            gdf.loc[gdf_subset.index, result_col] = interp_result_series

    def _interpolate_from_lookup_to_series_firstcut(self, lookup_df: pd.DataFrame, flooddepth_col: str, result_col: str):
        """
        Interpolates values from a lookup table, handles negative columns,
        and returns floor/ceil columns as separate Series, and adds
        the result to a specified column.

        Args:
            gdf: Geodataframe with a 'flooddepth' column.
            lookup_df: Dataframe with columns like 'ft04m', 'ft00', 'ft01', etc.
            flooddepth_col: Name of the flood depth column.
            result_col: Name of the column to store the interpolated result.

        Returns:
            gdf:  The input geodataframe with additional columns for
                floor_col, ceil_col, and result_col (interpolated value)
        """

        gdf = self.buildings.gdf

        # 1. Create a Mapping Dictionary
        def col_name_to_int(col_name):
            if col_name.endswith('m'):
                return -int(col_name[2:-1])
            else:
                return int(col_name[2:])

        col_to_int_map = {col: col_name_to_int(col) for col in lookup_df.columns}
        int_to_col_map = {v: k for k, v in col_to_int_map.items()}

        lookup_ints = np.array(sorted(col_to_int_map.values()))

        # Check if lookup cols are sequential from -N to N
        expected_ints = np.arange(lookup_ints.min(), lookup_ints.max() + 1)
        if not np.array_equal(lookup_ints, expected_ints):
            raise ValueError("Lookup df columns are not sequentially numbered from -N to N")

        # 2. Find Floor and Ceiling
        flood_values = gdf[flooddepth_col].to_numpy()
        insertion_points = np.searchsorted(lookup_ints, flood_values, side='left')
        insertion_points = np.clip(insertion_points, 1, len(lookup_ints) - 1)
        floor_indices = insertion_points - 1
        ceil_indices = insertion_points

        floor_ints = lookup_ints[floor_indices]
        ceil_ints = lookup_ints[ceil_indices]

        # Get the actual floor and ceil values from the flood depths
        floor_values_from_depth = np.floor(flood_values)
        # ceil_values_from_depth = np.ceil(flood_values)

        # Create Series (using the gdf's index!)
        floor_col_series = pd.Series(np.vectorize(int_to_col_map.get)(floor_ints), index=gdf.index)
        ceil_col_series = pd.Series(np.vectorize(int_to_col_map.get)(ceil_ints), index=gdf.index)

        # --- CRITICAL: Handle edge cases ---
        exact_matches = floor_values_from_depth == flood_values
        ceil_col_series[exact_matches] = floor_col_series[exact_matches]  # Directly modify Series

        # Handle when flood_values match the integer column name
        is_integer_match = np.isclose(flood_values, np.round(flood_values))

        floor_ints_adjusted = np.where(is_integer_match, floor_ints -1, floor_ints)
        floor_col_series = pd.Series(np.vectorize(int_to_col_map.get)(floor_ints_adjusted), index=gdf.index)

        # --- END CRITICAL: Handle edge cases ---

        # 3. Gather Corresponding Values
        lookup_array = lookup_df.to_numpy()
        row_indices = np.arange(len(gdf))

        floor_col_indices = np.array([lookup_df.columns.get_loc(col) for col in floor_col_series])
        ceil_col_indices = np.array([lookup_df.columns.get_loc(col) for col in ceil_col_series])

        floor_lookup_values = lookup_array[row_indices, floor_col_indices]
        ceil_lookup_values = lookup_array[row_indices, ceil_col_indices]

        # 4. Interpolate
        interp_factor = np.where(
            (ceil_ints - floor_ints) != 0,
            (flood_values - floor_ints) / (ceil_ints - floor_ints),
            0
        )
        interp_result_series = pd.Series(
            floor_lookup_values + interp_factor * (ceil_lookup_values - floor_lookup_values),
            index=gdf.index  # Use gdf's index
        )

        # Handle when flood_values is exactly equal to the integer col:
        gdf[result_col] = pd.Series(np.where(is_integer_match,
                                            ceil_lookup_values,
                                            interp_result_series), index = gdf.index)

        #  NO RETURN STATEMENT (Modifies gdf in-place)